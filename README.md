# AI-Powered Skill Gap Analyzer


## Problem
Applicants often get rejected without feedback. Keyword matchers are shallow and not explainable.

## Solution
A Retrieval-Augmented Generation system that:
- takes a resume + job description
- retrieves role expectations, skill benchmarks, and learning resources from a vector database
- uses an LLM to generate evidence-grounded skill gap insights, resume rewrites, and a learning plan

## Tech Stack
- FastAPI (backend)
- Streamlit (frontend)
- OpenAI (GPT-4o + embeddings)
- Pinecone (vector database)
- pdfplumber (resume PDF parsing)

## Setup
1. `pip install -r requirements.txt`
2. Set `.env` variables (see `.env.example`)
3. Seed Pinecone: `python scripts/seed_pinecone.py`
4. Run backend: `uvicorn backend.main:app --reload --port 8000`
5. Run UI: `streamlit run frontend/app.py`

## Ethics & Safety
- Advisory tool only; not a hiring decision system.
- Does not rank candidates.
- Avoids fabricating experience.
- Notes bias risks in job descriptions.
- No user data persistence by design.

## Future Work
- richer corpus (O*NET / ESCO skill taxonomy)
- stronger evaluation harness
- optional multi-agent pipeline (Analyzer/Editor/Planner)


Future Work

This project demonstrates a working Retrieval-Augmented Generation (RAG) pipeline for analyzing skill gaps between a resume and a job description. While the current system is correct and functional, several improvements would increase precision, robustness, and domain coverage:

Skill-Specific Benchmark Corpus
The current knowledge base contains role expectations by level (Entry/Mid/Senior). A more granular corpus could add benchmarks per skill (e.g., SQL window functions and optimization patterns, Airflow DAG design/backfills/SLAs, warehouse partitioning and cost control). This would enable more actionable and verifiable gap detection beyond general guidance.

Grounded Resume Rewrite Library
Resume improvement suggestions are currently generated by the LLM based on retrieved role expectations. Adding a curated set of resume rewrite examples (“before → after” bullets) would allow the system to retrieve proven rewrite patterns, improving consistency and realism across different roles and experience levels.

Industry-Specific Expectations
Different domains emphasize different engineering constraints (e.g., compliance and privacy constraints in HealthTech, auditability in FinTech, high-volume real-time systems in AdTech). Extending the corpus with industry expectation documents would help tailor the analysis to domain-specific hiring signals.

Curated Learning Resource Retrieval
The system currently recommends learning resources in a generic way. Adding short learning-resource snippets (with sources) to the vector store would allow the model to ground recommendations in curated materials, reducing hallucination risk and improving reproducibility of learning plans.

Reliability & Safety Guardrails
Future iterations can enforce “retrieval sufficiency” (refuse to generate analysis if no relevant documents are retrieved) and add structured evaluation metrics such as retrieval relevance thresholds, consistency checks, and regression tests on known resume/JD pairs.

These enhancements primarily improve precision and actionability while preserving the project’s core principle: grounded, explainable outputs backed by retrieval transparency.