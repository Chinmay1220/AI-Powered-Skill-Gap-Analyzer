{"id":"de_expectations_3","type":"role_expectation","role":"Data Engineer","level":"Senior","text":"Senior Data Engineers design scalable data architectures, set standards for data modeling and governance, lead reliability and observability practices, and drive cost/performance optimization. They mentor engineers, review designs, and handle complex migrations, privacy/security constraints, and cross-team dependencies."}
{"id":"de_expectations_modernstack_1","type":"role_expectation","role":"Data Engineer","level":"Entry","text":"Modern DE stacks often include dbt for transformations and Git-based workflows. Entry-level expectations may include writing modular SQL models, basic tests, documenting lineage, and collaborating through pull requests."}
{"id":"de_expectations_cloud_1","type":"role_expectation","role":"Data Engineer","level":"Mid","text":"Cloud data engineering commonly requires experience with warehouses (BigQuery/Snowflake/Redshift), orchestration (Airflow), object storage (S3/GCS), and data formats (Parquet/Avro). Mid-level engineers typically manage incremental models, partitioning strategies, and data quality checks."}
{"id":"de_expectations_streaming_1","type":"role_expectation","role":"Data Engineer","level":"Mid","text":"Streaming-oriented Data Engineers work with Kafka/Kinesis/PubSub, event schemas, consumer lag monitoring, and exactly-once/at-least-once tradeoffs. They build near-real-time pipelines and handle backpressure, replay, and late-arriving events."}
{"id":"mle_expectations_2","type":"role_expectation","role":"ML Engineer","level":"Mid","text":"Mid-level ML Engineers own model deployment and monitoring: serving (FastAPI), containerization (Docker), CI/CD, feature pipelines, evaluation automation, drift monitoring, and incident response. They work with data engineers to ensure reproducible training data and scalable inference."}
{"id":"mle_expectations_llm_1","type":"role_expectation","role":"ML Engineer","level":"Mid","text":"LLM-focused ML Engineers often build RAG systems: embeddings, vector databases, retrieval evaluation, prompt orchestration, and grounding. They implement safety filters and measure hallucination risk using test sets."}
{"id":"mle_expectations_platform_1","type":"role_expectation","role":"ML Engineer","level":"Senior","text":"Senior ML Engineers lead ML platform decisions, define best practices for experimentation, deployment, monitoring, and governance, and ensure models are reliable, secure, and compliant. They mentor teams and standardize pipelines across projects."}
{"id":"da_expectations_1","type":"role_expectation","role":"Data Analyst","level":"Entry","text":"Entry-level Data Analysts demonstrate SQL querying, dashboarding (Tableau/Power BI), basic statistics, and clear communication. They can translate business questions into metrics and produce concise insights with visualizations."}
{"id":"da_expectations_2","type":"role_expectation","role":"Data Analyst","level":"Mid","text":"Mid-level Data Analysts define KPIs, build reliable reporting pipelines, validate data quality, and run deeper analyses (cohorts, funnels, A/B testing). They partner with stakeholders to influence decisions and prioritize work."}
{"id":"skill_benchmark_python_1","type":"skill_benchmark","skill":"Python","text":"Benchmark: Strong Python evidence includes production-grade code, modular design, error handling, testing, type hints (optional), and data tooling familiarity (Pandas/Spark). Resume evidence: shipped features, performance improvements, automation impact, and reproducible scripts/pipelines."}
{"id":"skill_benchmark_data_modeling_1","type":"skill_benchmark","skill":"Data Modeling","text":"Benchmark: Data modeling includes designing fact/dimension tables, defining grain, handling slowly changing dimensions, enforcing constraints, and maintaining clear documentation. Resume evidence: star schema work, KPI definitions, and measurable improvements in query performance or reporting reliability."}
{"id":"skill_benchmark_dbt_1","type":"skill_benchmark","skill":"dbt","text":"Benchmark: dbt proficiency includes modular models, sources, tests, documentation, exposures, and CI integration. Evidence: incremental models, macros, lineage awareness, and strong naming conventions."}
{"id":"skill_benchmark_cloud_1","type":"skill_benchmark","skill":"Cloud Data Warehousing","text":"Benchmark: Cloud warehouse proficiency includes partitioning/clustering, cost controls, access policies, and performance tuning. Evidence: improved query cost/latency, set up datasets/projects, implemented governance, or optimized storage layouts."}
{"id":"skill_benchmark_docker_1","type":"skill_benchmark","skill":"Docker","text":"Benchmark: Docker proficiency includes writing Dockerfiles, building images, managing environment variables, and running services locally. Evidence: containerized an API or pipeline, enabled reproducible dev environments, or improved deployment reliability."}
{"id":"skill_benchmark_kafka_1","type":"skill_benchmark","skill":"Kafka","text":"Benchmark: Kafka evidence includes producing/consuming events, schema evolution (Avro/Protobuf), offset management, consumer groups, and monitoring lag. Evidence: built streaming pipelines, handled retries/replays, and ensured data correctness under load."}
{"id":"skill_benchmark_fastapi_1","type":"skill_benchmark","skill":"FastAPI","text":"Benchmark: FastAPI proficiency includes clean endpoints, request/response schemas (Pydantic), auth basics (JWT optional), error handling, and testing. Evidence: built an API service, structured codebase, and integrated with databases or external services."}
{"id":"skill_benchmark_ci_cd_1","type":"skill_benchmark","skill":"CI/CD","text":"Benchmark: CI/CD evidence includes automated tests, linting, build pipelines, deployment workflows, and safe release practices. Evidence: GitHub Actions/Jenkins pipelines, reduced deployment time, improved reliability, or introduced quality gates."}
{"id":"skill_benchmark_observability_1","type":"skill_benchmark","skill":"Observability","text":"Benchmark: Observability includes logging, metrics, alerting, tracing, SLAs/SLOs, and incident response. Evidence: set SLAs/alerts for pipelines, built dashboards, reduced failures, and created runbooks."}
{"id":"learning_resource_python","type":"learning_resource","skill":"Python","text":"Learning: Strengthen Python with 1) data pipelines using Pandas/Spark, 2) clean module structure, 3) error handling + logging, 4) unit tests for core functions. Project: build an ETL pipeline with config-driven connectors and evaluation scripts."}
{"id":"learning_resource_data_modeling","type":"learning_resource","skill":"Data Modeling","text":"Learning: Practice dimensional modeling (facts/dims, grain, SCD types), and implement a star schema in a warehouse. Project: build a small analytics mart with documented KPIs and dbt tests."}
{"id":"learning_resource_dbt","type":"learning_resource","skill":"dbt","text":"Learning: Build a dbt project with sources, staging, marts, tests, docs. Add incremental models and CI checks. Project: transform raw event data into a KPI dashboard-ready model."}
{"id":"learning_resource_cloud","type":"learning_resource","skill":"Cloud Data Warehousing","text":"Learning: Study partitioning/clustering strategies, cost monitoring, and access controls. Project: load Parquet data into BigQuery/Snowflake, optimize queries, and document cost/perf improvements."}
{"id":"learning_resource_docker","type":"learning_resource","skill":"Docker","text":"Learning: Containerize a FastAPI service with environment variables and compose. Project: run FastAPI + a small DB locally, add health checks, and document reproducible setup."}
{"id":"learning_resource_kafka","type":"learning_resource","skill":"Kafka","text":"Learning: Implement a producer/consumer pipeline, measure lag, and handle replays. Project: stream events into a warehouse staging table with deduplication and late-arrival handling."}
{"id":"learning_resource_fastapi","type":"learning_resource","skill":"FastAPI","text":"Learning: Build endpoints with Pydantic schemas, error handling, and simple auth. Project: create a REST API that powers a Streamlit UI and returns structured JSON results."}
{"id":"learning_resource_ci_cd","type":"learning_resource","skill":"CI/CD","text":"Learning: Add GitHub Actions to run tests + lint on push. Project: build a pipeline that checks formatting, runs unit tests, and produces a deployable artifact."}
{"id":"learning_resource_observability","type":"learning_resource","skill":"Observability","text":"Learning: Add structured logging + basic metrics to your pipeline/API. Project: define SLAs for jobs, create alerts for failures, and write a runbook for incidents."}
